version: "3"
services:
  #------------------------------------------------------------------------------------------------
  # hadoop cluster
  #------------------------------------------------------------------------------------------------
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    networks:
      - backend
    restart: on-failure
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9861 datanode2:9862 datanode3:9863 "
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
    env_file:
      - ./hadoop.env
    ports: 
     - "8089:8088"

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    networks:
      - backend
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9861 datanode2:9862 datanode3:9863 resourcemanager:8088"
    depends_on: 
      - namenode
      - datanode1
      - datanode2
    volumes:
      - ./data/historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    ports: 
      - "8188:8188"

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager1
    networks:
      - backend
    environment:
     SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9861 datanode2:9862 datanode3:9863 resourcemanager:8088"
    depends_on: 
      - namenode
      - datanode1
      - datanode2
    env_file:
      - ./hadoop.env
    ports: 
      - "8042:8042"

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    hostname: datanode1
    networks:
      - backend
    volumes:
      - ./data/datanode1:/hadoop/dfs/data
    environment:
     SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    env_file:
      - ./hadoop.env
    environment: 
      HDFS_CONF_dfs_datanode_http_address: "0.0.0.0:9861"
    ports:
      - 9861:9861

  datanode2:
    # build: ./datanode
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    hostname: datanode2
    networks:
      - backend
    volumes:
      - ./data/datanode2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    env_file:
      - ./hadoop.env
    environment: 
      HDFS_CONF_dfs_datanode_http_address: "0.0.0.0:9862"
    ports:
      - 9862:9862

  datanode3:
    # build: ./datanode
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode3
    hostname: datanode3
    networks:
      - backend
    volumes:
      - ./data/datanode3:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    env_file:
      - ./hadoop.env
    environment: 
      HDFS_CONF_dfs_datanode_http_address: "0.0.0.0:9863"
    ports:
      - 9863:9863

  namenode:
    # build: ./namenode
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    networks:
      - backend
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=rudi_das_ruesselcluster
    env_file:
      - ./hadoop.env


  #------------------------------------------------------------------------------------------------
  # spark cluster
  #------------------------------------------------------------------------------------------------
  master:
      image: bde2020/spark-master:3.1.1-hadoop3.2
      container_name: master
      env_file: ../variables.env
      ports:
          - "7077:7077"
          - "8080:8080"
          - "4040:4040"
      networks:
          - backend

  worker-1:
      image: bde2020/spark-worker:3.1.1-hadoop3.2
      container_name: worker-1
      depends_on:
          - master
      ports:
          - "8081:8081"
      environment:
          - "SPARK_MASTER=spark://master:7077"
      networks:
          - backend
      #extra_hosts:
      # - "datalake6:192.168.220.6" # change to your hostname and ip address
  worker-2:
      image: bde2020/spark-worker:3.1.1-hadoop3.2
      container_name: worker-2
      depends_on:
          - master
      ports:
          - "8082:8082"
      environment:
          - "SPARK_MASTER=spark://master:7077"
      networks:
          - backend
      #extra_hosts:
      # - "datalake6:192.168.220.6" # change to your hostname and ip address


  #------------------------------------------------------------------------------------------------
  # databases
  #------------------------------------------------------------------------------------------------
  mongodb:
    image: mongo:4.2-bionic
    container_name: mongodb
    command: mongod
    networks:
      - backend
    ports:
      - 27017:27017
    env_file: ../variables.env
    volumes:
      - ./data/mongodb:/data/db

  postgres:
    image: postgres
    container_name: postgres
    networks:
      - backend
    ports:
      - 5432:5432
    env_file: ../variables.env
    volumes: 
      - ./data/postgres:/var/lib/postgresql/data

  fuseki:
    image: stain/jena-fuseki
    container_name: fuseki
    ports:
      - 3030:3030
    networks:
      - backend
    env_file: ../variables.env
    volumes:
      - ../config.ttl:/fuseki/config.ttl
      # - ../propertyorattribute.n3

networks:
  backend:
    external: false
